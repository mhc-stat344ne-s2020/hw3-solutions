{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssyEy81nY67K",
        "colab_type": "text"
      },
      "source": [
        "We're going to work with another of the examples from Chapter 3 of Chollet in this assignment.  Previously we used an example where the response was categorical with two categories; in this example, the response has 46 categories.\n",
        "\n",
        "Our goal is to use the text of newswires (a very short summary of a news article) to classify them into one of 46 different topics.  The topics are things like 'cocoa', 'grain', 'earn', 'housing', 'money-supply', 'nat-gas', and so on.  In this data set, each newswire only belongs to one of these classes.\n",
        "\n",
        "I have once again taken all of the code below to preprocess the data straight out of Chapter 3 of Chollet.  The book describes how the code works in some detail, so I encourage you to give it a read.\n",
        "\n",
        "### Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjIHVO6aJQg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "395dda93-5610-43a8-f508-ce8dc88d3181"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.datasets import reuters\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "import copy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXUwnHKlM2m",
        "colab_type": "text"
      },
      "source": [
        "### Load and preprocess the data\n",
        "\n",
        "This will take a few seconds to run; it downloads a large data set.\n",
        "\n",
        "The the newswires are one-hot encoded using the exact same representation as was used for the IMDB reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyXiHGfW3e1j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "35d2cd79-73e5-4ca1-932e-54ef41ce99af"
      },
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "\n",
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 2s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVGouITLF_G3",
        "colab_type": "text"
      },
      "source": [
        "Let's look at one of the newswires to see what it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOoHw1wBCxoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ccde75d2-b27b-4b86-da4a-ad548400517c"
      },
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
        "    train_data[0]])\n",
        "print(decoded_newswire)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 1s 2us/step\n",
            "? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb4zBhhGGRkn",
        "colab_type": "text"
      },
      "source": [
        "### 1. Define a Keras model and fit it.\n",
        "\n",
        "I'm making this an exercise to force you to get more practice with defining and fitting models in Keras, but note that the solution is in Chapter 3 of Chollet in case you get stuck...\n",
        "\n",
        "Our model should have the following structure:\n",
        "\n",
        "* Input layer has 10000 features (we kept the 10000 most commonly occuring words, and they are one-hot encoded).\n",
        "* A fully connected (in other words, dense) hidden layer with 64 units and relu activation.\n",
        "* A second fully connected (in other words, dense) hidden layer with 64 units and relu activation.\n",
        "* A fully connected output layer with number of units equal to the number of classes for the response and a softmax activation.\n",
        "\n",
        "Compile the model using `rmsprop` as the optimizer, 'categorical_crossentropy' as the loss, and ['accuracy'] as the only additional metric.\n",
        "\n",
        "Fit the model using `partial_x_train` and `partial_y_train` as the training data, 20 epochs, a `batch_size` of 512, and `validation_data` given by `(x_val, y_val)`.  Save the model fit history in an object called `history` so that we can plot the training and validation set accuracy later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v3ZYAg6mwJr",
        "colab_type": "code",
        "outputId": "5ed8b760-06a4-4514-8e60-92523c0eab32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7982/7982 [==============================] - 10s 1ms/step - loss: 2.7011 - acc: 0.5023 - val_loss: 1.8040 - val_acc: 0.6290\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 1.4649 - acc: 0.6964 - val_loss: 1.3439 - val_acc: 0.7160\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 1.0721 - acc: 0.7739 - val_loss: 1.1649 - val_acc: 0.7450\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.8333 - acc: 0.8250 - val_loss: 1.0513 - val_acc: 0.7720\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.6623 - acc: 0.8577 - val_loss: 0.9933 - val_acc: 0.7800\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.5237 - acc: 0.8914 - val_loss: 0.9229 - val_acc: 0.8080\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.4235 - acc: 0.9153 - val_loss: 0.9133 - val_acc: 0.8100\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.3427 - acc: 0.9326 - val_loss: 0.8989 - val_acc: 0.8150\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.2851 - acc: 0.9404 - val_loss: 0.9184 - val_acc: 0.8090\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2364 - acc: 0.9458 - val_loss: 0.9283 - val_acc: 0.8120\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.2031 - acc: 0.9479 - val_loss: 0.9194 - val_acc: 0.8120\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1822 - acc: 0.9523 - val_loss: 0.9346 - val_acc: 0.8050\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1648 - acc: 0.9513 - val_loss: 0.9642 - val_acc: 0.8110\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1461 - acc: 0.9559 - val_loss: 0.9513 - val_acc: 0.8180\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1395 - acc: 0.9564 - val_loss: 0.9885 - val_acc: 0.8150\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 0s 55us/step - loss: 0.1314 - acc: 0.9539 - val_loss: 1.0002 - val_acc: 0.8100\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1230 - acc: 0.9568 - val_loss: 1.0205 - val_acc: 0.8100\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 0s 56us/step - loss: 0.1188 - acc: 0.9567 - val_loss: 1.0491 - val_acc: 0.8040\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 0s 54us/step - loss: 0.1155 - acc: 0.9575 - val_loss: 1.1210 - val_acc: 0.7830\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 0s 57us/step - loss: 0.1111 - acc: 0.9579 - val_loss: 1.0856 - val_acc: 0.8030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVgnF1odm0wj",
        "colab_type": "text"
      },
      "source": [
        "### Plot model estimation history\n",
        "\n",
        "You don't have to do anything here other than run this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wr1aZ_TomwMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2411ae06-5fdf-486c-ac51-25766e537c37"
      },
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zecReazzJWCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "cfea0057-4576-4690-8ca5-03ad3821aa0a"
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mojt2WoAhOuE",
        "colab_type": "text"
      },
      "source": [
        "### 2. Explain how you can tell the model has overfit the training data from the plots of training and validation set performance above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6liLKPmhZlO",
        "colab_type": "text"
      },
      "source": [
        "(add your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNIU7KkdmmCD",
        "colab_type": "text"
      },
      "source": [
        "### Get test set performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTAkK_wwkOOQ",
        "colab_type": "code",
        "outputId": "103f67d6-e4f9-4cdb-d92d-07bbad3d971c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(x_test, one_hot_test_labels)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2246/2246 [==============================] - 0s 90us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2110338274751087, 0.7827248442204849]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5xe5yjfjDlj",
        "colab_type": "text"
      },
      "source": [
        "For comparison, here's the accuracy of an approach that randomly shuffles the test set labels (basically, how well would we do if we just made predictions at random?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRwHjfjwjQcP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91148ca3-33db-4d5b-e841-7c4eb64ff204"
      },
      "source": [
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
        "float(np.sum(hits_array)) / len(test_labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1981300089047195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaY2Z5o_mo8n",
        "colab_type": "text"
      },
      "source": [
        "### Transpose/reshape data so that it's shaped the way our manual implementation expects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVTtdrfK7dS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "partial_x_train = partial_x_train.T\n",
        "partial_y_train = partial_y_train.T\n",
        "x_val = x_val.T\n",
        "y_val = y_val.T\n",
        "x_test = x_test.T\n",
        "y_test = one_hot_test_labels.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUPB3fyq9hfR",
        "colab_type": "code",
        "outputId": "caca4a89-8c0a-4073-b423-6c28d71b6d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(partial_x_train.shape)\n",
        "print(partial_y_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 7982)\n",
            "(46, 7982)\n",
            "(10000, 1000)\n",
            "(46, 1000)\n",
            "(10000, 2246)\n",
            "(46, 2246)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chfeW2nwoexP",
        "colab_type": "text"
      },
      "source": [
        "### Model in NumPy\n",
        "\n",
        "Again, we're going to code this model up from scratch.  We will implement 5 functions:\n",
        "1. `initialize_params` will generate random starting values for the parameters b and w.\n",
        "2. `forward_prop` does the calculations for forward propagation.\n",
        "3. `backward_prop` does the calculations for backward propagation.\n",
        "4. `grad_check` does gradient checking so that we can be sure our backward propagation is correct.  I've written this one.\n",
        "5. `fit_model` does gradient descent to estimate the model parameters.\n",
        "\n",
        "Note that as defined above, our network has 3 layers (after the input layer):\n",
        " * Layer 1 has 64 units and a relu activation\n",
        " * Layer 2 has 64 units and a relu activation\n",
        " * Layer 3 has 46 unit and a softmax activation\n",
        "\n",
        "### 1. initialize_params\n",
        "\n",
        "Add the appropriate shape specifications to the initializations below.\n",
        "\n",
        "**Note that you are defining $w$, not $w^T$**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-dyx8N_8Mkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_params(num_features, seed = 9433):\n",
        "  '''\n",
        "  Initialize parameter values for a network with 3 layers:\n",
        "  layer 1 has 64 units, layer 2 has 64 units, and layer 3 has 46 units\n",
        "\n",
        "  Arguments:\n",
        "    - num_features: number of input features\n",
        "    - seed: seed to use for random number generation\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary with initial values for b1, w1, b2, w2, b3, and w3\n",
        "  '''\n",
        "  # set seed\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # layer 1 parameters -- replace Nones with appropriate shape\n",
        "  b1 = np.random.standard_normal((64, 1)) * 0.1 \n",
        "  w1 = np.random.standard_normal((num_features, 64)) * 0.1\n",
        "\n",
        "  # layer 2 parameters -- replace Nones with appropriate shape\n",
        "  b2 = np.random.standard_normal((64, 1)) * 0.1\n",
        "  w2 = np.random.standard_normal((64, 64)) * 0.1\n",
        "\n",
        "  # layer 3 parameters -- replace Nones with appropriate shape\n",
        "  b3 = np.random.standard_normal((46, 1)) * 0.1\n",
        "  w3 = np.random.standard_normal((64, 46)) * 0.1\n",
        "\n",
        "  return({\n",
        "      'b1': b1,\n",
        "      'w1': w1,\n",
        "      'b2': b2,\n",
        "      'w2': w2,\n",
        "      'b3': b3,\n",
        "      'w3': w3\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffbcK7QZrkiq",
        "colab_type": "text"
      },
      "source": [
        "### 2. forward_prop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkIvAcc7-DRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(params, x):\n",
        "  '''\n",
        "  Forward propagation calculations\n",
        "\n",
        "  Arguments:\n",
        "    - params: dictionary with values for b1, w1, b2, w2, b3, w3\n",
        "    - x: matrix of features, shape (p, m)\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary with z and a values for each layer\n",
        "  '''\n",
        "  # Pull out parameters from params dictionary\n",
        "  b1 = params['b1']\n",
        "  w1 = params['w1']\n",
        "  b2 = params['b2']\n",
        "  w2 = params['w2']\n",
        "  b3 = params['b3']\n",
        "  w3 = params['w3']\n",
        "  \n",
        "  # Calculate forward propagation\n",
        "  # You can use np.maximum(0, ___) to compute the relu activation function\n",
        "  # We have imported the softmax function\n",
        "  z1 = b1 + np.dot(w1.T, x)\n",
        "  a1 = np.maximum(0, z1)\n",
        "\n",
        "  z2 = b2 + np.dot(w2.T, a1)\n",
        "  a2 = np.maximum(0, z2)\n",
        "\n",
        "  z3 = b3 + np.dot(w3.T, a2)\n",
        "  a3 = softmax(z3, axis = 0)\n",
        "  \n",
        "  # Return dictionary of results from forward propagation\n",
        "  return({\n",
        "    'z1': z1,\n",
        "    'a1': a1,\n",
        "    'z2': z2,\n",
        "    'a2': a2,\n",
        "    'z3': z3,\n",
        "    'a3': a3\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-BSu3vhtG6J",
        "colab_type": "text"
      },
      "source": [
        "### 3. backward_prop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLIPXARSALF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_prop(params, x, y, forward_cache):\n",
        "  '''\n",
        "  Backward propagation calculations\n",
        "\n",
        "  Arguments:\n",
        "    - params: dictionary with values for b1, w1, b2, w2, b3, w3\n",
        "    - x: array of features, shape (p, m)\n",
        "    - y: array of responses, shape (1, m)\n",
        "    - forward_cache: Dictionary with z and a values for each layer\n",
        "      (return value from forward_prop)\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary of derivatives of cost function J with respect to\n",
        "      b1, w1, b2, w2, b3, w3\n",
        "  '''\n",
        "  # Extract quantities needed for backward propagation calculations\n",
        "  m = x.shape[1]\n",
        "\n",
        "  z1 = forward_cache['z1']\n",
        "  a1 = forward_cache['a1']\n",
        "  z2 = forward_cache['z2']\n",
        "  a2 = forward_cache['a2']\n",
        "  z3 = forward_cache['z3']\n",
        "  a3 = forward_cache['a3']\n",
        "\n",
        "  w1 = params['w1']\n",
        "  w2 = params['w2']\n",
        "  w3 = params['w3']\n",
        "\n",
        "  # Backward propagation calculations\n",
        "  # Layer 3\n",
        "  dJdz3 = a3 - y\n",
        "  dJdb3 = np.mean(dJdz3, axis = 1, keepdims=True)\n",
        "  dJdw3 = (1/m) * np.dot(a2, dJdz3.T)\n",
        "\n",
        "  # Layer 2\n",
        "  dJda2 = np.dot(w3, dJdz3)\n",
        "  dJdz2 = dJda2 * (z2 >= 0).astype(float)\n",
        "  dJdb2 = np.mean(dJdz2, axis = 1, keepdims=True)\n",
        "  dJdw2 = (1/m) * np.dot(a1, dJdz2.T)\n",
        "\n",
        "  # Layer 1\n",
        "  dJda1 = np.dot(w2, dJdz2)\n",
        "  dJdz1 = dJda1 * (z1 >= 0).astype(float)\n",
        "  dJdb1 = np.mean(dJdz1, axis = 1, keepdims=True)\n",
        "  dJdw1 = (1/m) * np.dot(x, dJdz1.T)\n",
        "\n",
        "  return({\n",
        "      'dJdb1': dJdb1,\n",
        "      'dJdw1': dJdw1,\n",
        "      'dJdb2': dJdb2,\n",
        "      'dJdw2': dJdw2,\n",
        "      'dJdb3': dJdb3,\n",
        "      'dJdw3': dJdw3\n",
        "  })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_tep-avtLXd",
        "colab_type": "text"
      },
      "source": [
        "### 4. grad_check\n",
        "I have implemented this in the two functions below; you just need to run the code.  After the function definitions, there are calls to check the gradient for a bias parameter and a weight parameter in each of the 3 layers.  Your results should match to at least 6 or 7 decimal places.\n",
        "\n",
        "Although I have implemented this, you should know how this works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCvqYgt1FfB5",
        "colab_type": "code",
        "outputId": "166193e8-1a38-4ca9-e66d-1641b18c8172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "def loss(a3, y):\n",
        "  '''\n",
        "  Calculate the binary crossentropy loss\n",
        "\n",
        "  Arguments:\n",
        "    - a3: output from layer 3\n",
        "    - y: observed responses\n",
        "  \n",
        "  Return:\n",
        "    - binary cross entropy\n",
        "  '''\n",
        "  m = y.shape[1]\n",
        "  result = 0\n",
        "  for y_value in range(46):\n",
        "    inds = np.where(y[y_value, :] == 1.)\n",
        "    result += np.sum(np.log(a3[y_value, inds]))\n",
        "\n",
        "  return(-1 * result / m)\n",
        "\n",
        "def grad_check(param_name, ind1, ind2, eps = 0.000001):\n",
        "  '''\n",
        "  Calculate gradient check by finite differencing\n",
        "\n",
        "  Arguments:\n",
        "    - param_name: name of parameter to check, e.g. 'w1'\n",
        "    - ind1, ind2: indices to check for the specified parameter\n",
        "    - eps: amount to add and subtract from the given parameter value\n",
        "  \n",
        "  Returns:\n",
        "    - No return value.  Prints diagnostic messages.\n",
        "  '''\n",
        "  params = initialize_params(num_features = partial_x_train.shape[0])\n",
        "  forward_cache = forward_prop(params, partial_x_train)\n",
        "\n",
        "  params[param_name][ind1, ind2] = params[param_name][ind1, ind2] + eps\n",
        "  forward_cache_plus_eps = forward_prop(params, partial_x_train)\n",
        "  loss_plus_eps = loss(forward_cache_plus_eps['a3'], partial_y_train)\n",
        "\n",
        "  params[param_name][ind1, ind2] = params[param_name][ind1, ind2] - 2*eps\n",
        "  forward_cache_minus_eps = forward_prop(params, partial_x_train)\n",
        "  loss_minus_eps = loss(forward_cache_minus_eps['a3'], partial_y_train)\n",
        "\n",
        "  est_grad = (loss_plus_eps - loss_minus_eps) / (2 * eps)\n",
        "  calc_grad = backward_prop(params, partial_x_train, partial_y_train, forward_cache)\n",
        "\n",
        "  print(\"\\ncheck of dJd\" + param_name + \"[\" + str(ind1) + \", \" + str(ind2) + \"]\")\n",
        "  print(\"estimated derivative = \" + str(est_grad))\n",
        "  print(\"calculated derivative = \" + str(calc_grad['dJd' + param_name][ind1, ind2]))\n",
        "\n",
        "\n",
        "grad_check('b3', 0, 0)\n",
        "grad_check('w3', 1, 0)\n",
        "grad_check('b2', 0, 0)\n",
        "grad_check('w2', 1, 0)\n",
        "grad_check('b1', 0, 0)\n",
        "grad_check('w1', 1, 0)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "check of dJdb3[0, 0]\n",
            "estimated derivative = 0.011951571288193463\n",
            "calculated derivative = 0.011951571483176356\n",
            "\n",
            "check of dJdw3[1, 0]\n",
            "estimated derivative = 0.0031107705300570387\n",
            "calculated derivative = 0.0031107697530570436\n",
            "\n",
            "check of dJdb2[0, 0]\n",
            "estimated derivative = 0.0003886091448634943\n",
            "calculated derivative = 0.0003886100569395781\n",
            "\n",
            "check of dJdw2[1, 0]\n",
            "estimated derivative = -0.00019852941512965572\n",
            "calculated derivative = -0.00019852902154475605\n",
            "\n",
            "check of dJdb1[0, 0]\n",
            "estimated derivative = -0.004426695898729349\n",
            "calculated derivative = -0.004426695243287879\n",
            "\n",
            "check of dJdw1[1, 0]\n",
            "estimated derivative = -0.004426695898729349\n",
            "calculated derivative = -0.004426695243287879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAedPjUhvR_t",
        "colab_type": "text"
      },
      "source": [
        "### 5. fit_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb7sSkKuz0xo",
        "colab_type": "text"
      },
      "source": [
        "### Fit model by Stochastic Gradient Descent\n",
        "\n",
        "Below is almost the exact function from our lab implementing stochastic gradient descent -- I took out the training set accuracy calculation to save a little time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o2RhfxEaE9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model_SGD(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    num_epochs,\n",
        "    minibatch_size,\n",
        "    learning_rate,\n",
        "    initial_params):\n",
        "  '''\n",
        "  Estimate model parameters by stochastic gradient descent (SGD)\n",
        "\n",
        "  Arguments:\n",
        "    - x_train: array of input features of shape (p, m)\n",
        "    - y_train: array of responses of shape (1, m)\n",
        "    - num_epochs: number of iterations of gradient descent to run\n",
        "    - minibatch_size: number of observations in each minibatch\n",
        "    - learning_rate: learning rate for gradient descent\n",
        "    - initial_params: dictionary of starting parameter values\n",
        "  \n",
        "  Return:\n",
        "    - Dictionary of parameter estimates b1, w1, b2, w2, b3, w3\n",
        "  '''\n",
        "  params = initial_params\n",
        "\n",
        "  # calculate size of each minibatch\n",
        "  m = x_train.shape[1]\n",
        "  num_minibatches = m // minibatch_size + 1\n",
        "\n",
        "  # for loop over the number of epochs\n",
        "  for i in range(num_epochs):\n",
        "    print(\"\\nepoch \" + str(i))\n",
        "    # for loop over minibatches within the current epoch\n",
        "    for j in range(num_minibatches):\n",
        "      # print progress indicator.  end = \"\" makes it so there is not a new line\n",
        "      # after each print statement\n",
        "      print(\".\", end = \"\")\n",
        "\n",
        "      # Set up a slice for the observation indices in the current minibatch\n",
        "      # Code to do this will be different depending on whether you're in the\n",
        "      # last minibatch or not.\n",
        "      if j == num_minibatches - 1:\n",
        "        minibatch_inds = slice(j*minibatch_size, m)\n",
        "      else:\n",
        "        minibatch_inds = slice(j*minibatch_size, (j+1)*minibatch_size)\n",
        "      \n",
        "      # pull out the x and y observations for this minibatch\n",
        "      x_train_minibatch = x_train[:, minibatch_inds]\n",
        "      y_train_minibatch = y_train[:, minibatch_inds]\n",
        "\n",
        "      # forward propagation based on this minibatch\n",
        "      forward_cache = forward_prop(params, x_train_minibatch)\n",
        "\n",
        "      # backward propagation based on this minibatch\n",
        "      calc_grad = backward_prop(params, x_train_minibatch, y_train_minibatch, forward_cache)\n",
        "\n",
        "      # gradient descent updates based on thsi minibatch\n",
        "      params['b1'] = params['b1'] - learning_rate * calc_grad['dJdb1']\n",
        "      params['w1'] = params['w1'] - learning_rate * calc_grad['dJdw1']\n",
        "      params['b2'] = params['b2'] - learning_rate * calc_grad['dJdb2']\n",
        "      params['w2'] = params['w2'] - learning_rate * calc_grad['dJdw2']\n",
        "      params['b3'] = params['b3'] - learning_rate * calc_grad['dJdb3']\n",
        "      params['w3'] = params['w3'] - learning_rate * calc_grad['dJdw3']\n",
        "  \n",
        "  # return parameters estimates\n",
        "  return(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqtntev81T1t",
        "colab_type": "text"
      },
      "source": [
        "Now we can call the `fit_model_SGD` function to do stochastic gradient descent for our data set using 40 epochs, a minibatch size of 512 observations, and a learning rate of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTiuDwMCdDO8",
        "colab_type": "code",
        "outputId": "1b425686-855c-4c51-fedb-c462f45f1bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# generate starting values for the parameters\n",
        "initial_params = initialize_params(num_features = partial_x_train.shape[0])\n",
        "\n",
        "# do estimation\n",
        "params_fit_mb = fit_model_SGD(\n",
        "  partial_x_train,\n",
        "  partial_y_train,\n",
        "  num_epochs=40,\n",
        "  minibatch_size=512,\n",
        "  learning_rate=0.1,\n",
        "  initial_params = initial_params)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "epoch 0\n",
            "................\n",
            "epoch 1\n",
            "................\n",
            "epoch 2\n",
            "................\n",
            "epoch 3\n",
            "................\n",
            "epoch 4\n",
            "................\n",
            "epoch 5\n",
            "................\n",
            "epoch 6\n",
            "................\n",
            "epoch 7\n",
            "................\n",
            "epoch 8\n",
            "................\n",
            "epoch 9\n",
            "................\n",
            "epoch 10\n",
            "................\n",
            "epoch 11\n",
            "................\n",
            "epoch 12\n",
            "................\n",
            "epoch 13\n",
            "................\n",
            "epoch 14\n",
            "................\n",
            "epoch 15\n",
            "................\n",
            "epoch 16\n",
            "................\n",
            "epoch 17\n",
            "................\n",
            "epoch 18\n",
            "................\n",
            "epoch 19\n",
            "................\n",
            "epoch 20\n",
            "................\n",
            "epoch 21\n",
            "................\n",
            "epoch 22\n",
            "................\n",
            "epoch 23\n",
            "................\n",
            "epoch 24\n",
            "................\n",
            "epoch 25\n",
            "................\n",
            "epoch 26\n",
            "................\n",
            "epoch 27\n",
            "................\n",
            "epoch 28\n",
            "................\n",
            "epoch 29\n",
            "................\n",
            "epoch 30\n",
            "................\n",
            "epoch 31\n",
            "................\n",
            "epoch 32\n",
            "................\n",
            "epoch 33\n",
            "................\n",
            "epoch 34\n",
            "................\n",
            "epoch 35\n",
            "................\n",
            "epoch 36\n",
            "................\n",
            "epoch 37\n",
            "................\n",
            "epoch 38\n",
            "................\n",
            "epoch 39\n",
            "................"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKswX8LrDV_l",
        "colab_type": "text"
      },
      "source": [
        "Below, do the necessary calculations to find the test set prediction accuracy.  You will want to use the `np.argmax` function as you did on HW2.  Note that you will probably want to do this both to get y_test_hat and also to get a sparse representation of y_test to compare to when determining your test set accuracy.  Your test set accuracy is still not as good as what was achieved using Keras above, but it is not bad!  You could run estimation for longer to do even better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5-YGjBQjzc6",
        "colab_type": "code",
        "outputId": "994c0fad-2398-4741-e485-353eae64caaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# call forward propagation to get the activation output from the network\n",
        "# based on the parameter estimates param_fit_mb and the test set features x_test\n",
        "test_forward = forward_prop(params_fit_mb, x_test)\n",
        "\n",
        "# get estimated y_test using np.argmax\n",
        "y_test_hat = np.argmax(test_forward['a3'], axis = 0)\n",
        "\n",
        "# get a sparse version of y_test using np.argmax\n",
        "y_test_sparse = np.argmax(y_test, axis = 0)\n",
        "\n",
        "# get test set accuracy by comparing y_test_hat and y_test_sparse\n",
        "np.mean(y_test_hat == y_test_sparse)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7172751558325913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}